# ğŸ•·ï¸ Scrapy Project: Test1

Welcome to the **Test1 Scrapy Project**! ğŸš€ This project is designed to scrape data from websites using the powerful [Scrapy](https://scrapy.org/) framework. It includes custom spiders, middleware, and configurations to make web scraping efficient and scalable. ğŸ‰

---

## ğŸ“‚ Project Structure

Here's an overview of the project structure:

```
test1/
â”œâ”€â”€ test1/items.py          # Define the data structure for scraped items
â”œâ”€â”€ test1/middlewares.py    # Custom middleware for processing requests and responses
â”œâ”€â”€ pipelines.py            # Define pipelines to process scraped data
â”œâ”€â”€ settings.py             # Project settings for Scrapy
â”œâ”€â”€ spiders/                # Directory containing all spiders
â”‚   â”œâ”€â”€ test_spider.py      # Example spider for testing
â”‚   â””â”€â”€ AmazonJobsSpider.py # Spider for scraping Amazon job listings
scrapy.cfg                  # Scrapy configuration file
```

---

## ğŸ› ï¸ Features

- **Custom Spiders** ğŸ•·ï¸: Includes spiders like `TestSpider` and `AmazonJobsSpider` for targeted scraping.
- **Middleware** ğŸ”§: Custom middleware to handle requests and responses efficiently.
- **Playwright Integration** ğŸ­: Uses `scrapy-playwright` for handling JavaScript-heavy websites.
- **Data Pipelines** ğŸ“¦: Process and store scraped data seamlessly.

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Prerequisites

Ensure you have the following installed:

- Python 3.10+ ğŸ
- Scrapy ğŸ•·ï¸
- Playwright ğŸ­

### 2ï¸âƒ£ Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd test1
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Install Playwright browsers:
   ```bash
   playwright install
   ```

### 3ï¸âƒ£ Running a Spider

Run the `test_spider` to test the setup:
```bash
scrapy crawl test_spider
```

Run the `AmazonJobsSpider` to scrape job listings:
```bash
scrapy crawl AmazonJobsSpider
```

---

## âš™ï¸ Configuration

### Scrapy Settings

The project settings are defined in `test1/settings.py`. Key configurations include:

- **User-Agent**: Customize the user agent for requests.
- **Middleware**: Enable `PlaywrightMiddleware` for JavaScript rendering.
- **Pipelines**: Configure data pipelines for processing scraped items.

### Playwright Integration

Ensure `scrapy-playwright` is installed and configured in the settings:
```python
DOWNLOAD_HANDLERS = {
    "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
    "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
}
```

---

## ğŸ“Š Output

Scraped data can be exported in various formats:
```bash
scrapy crawl <spider_name> -o output.json
```

---

## ğŸ› Troubleshooting

- **ModuleNotFoundError**: Ensure `scrapy-playwright` is installed:
  ```bash
  pip install scrapy-playwright
  ```

- **Playwright Issues**: Ensure Playwright browsers are installed:
  ```bash
  playwright install
  ```

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to open issues or submit pull requests. ğŸ› ï¸

---

## ğŸ“œ License

This project is licensed under the MIT License. ğŸ“„

---

## ğŸŒŸ Acknowledgments

- [Scrapy](https://scrapy.org/)
- [Playwright](https://playwright.dev/)

Happy Scraping! ğŸ‰